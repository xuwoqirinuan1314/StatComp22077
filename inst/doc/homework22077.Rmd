---
title: "Homework of 22077"
author: "22077"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework of 22077}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# A-22077-2022-09-09

## Question

Use knitr to produce at least 3 examples(texts,figures,tables)

## Answer

texts:

This is an R Markdown presentation. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. 

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.

figures:

```{r}
plot(iris)
```

tables:

```{r}
data(iris)
head(iris,15)
```

# A-22077-2022-09-15
## Exercises 3.3
```{r}
n <- 1000
u <- runif(n)
x <- 2*(1-u)^(-1/2)
hist(x, prob = TRUE,breaks = 1000,xlim = c(2,8),main = expression(f(x)==8*x^(-3)),border = "red") #density histogram of sample
y <- seq(2, 8, .001)
lines(y, 8*y^(-3))
#density curve f(x)
```

## Exercise 3.7

```{r}
n <- 1e4;j<-k<-0;y <- numeric(n)
while (k < n) {
  u <- runif(1)
  j <- j + 1
  x <- runif(1) #random variate from g(.)
  if ((16/9)*x^2*(1-x) > u) {
    #we accept x
    k <- k + 1
    y[k] <- x
  }
}
hist(y, prob = TRUE,breaks = 50,border = "red",main = expression(f(x)==12*x^2*(1-x)),)
z <- seq(0, 1, .01)
lines(z, 12*z^2*(1-z))
#density curve f(x)
```

## Exercise 3.12

```{r}
n <- 1e3; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
x <- rexp(n, lambda) # the length of lambda = n
hist(x,breaks = 400,xlim = c(0,5),borde = "red")
```

## Exercise 3.13

```{r}
n <- 1e4; r <- 4; beta <- 2
lambda <- rgamma(n, r, beta)
x <- rexp(n, lambda) # the length of lambda = n
hist(x,prob = TRUE,breaks = 400,xlim = c(0,5),borde = "red",main = expression(f(x)==64*(2+x)^(-5)))
z <- seq(0, 5, .01)
lines(z, 64*(2+z)^(-5))
```

# A-22077-2022-09-23
## Qusetion 

1. For $n = 10^4$, $2 × 10^4$, $4 × 10^4$, $6 × 10^4$, $8 × 10^4$, apply the fast sorting algorithm to randomly permuted numbers of 1,..., n.
(1)Calculate computation time averaged over 100 simulations, denoted by an.
(2)Regress an on tn := n log(n), and graphically show the results (scatter plot and regression line).
2. Exercises 5.6, 5.7(pages 149-151, Statistical Computing with R).

## Answer

Question 1

```{r}
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}
test<-sample(1:1e4)
ans <- 0               
for (i in 1:100) {
  w<-system.time(quick_sort(test))[1]
  ans <- ans + w    
}
a1=ans/100
a1

test<-sample(1:2e4)
ans <- 0               
for (i in 1:100) {
  w<-system.time(quick_sort(test))[1]
  ans <- ans + w    
}
a2=ans/100
a2

test<-sample(1:4e4)
ans <- 0               
for (i in 1:100) {
  w<-system.time(quick_sort(test))[1]
  ans <- ans + w    
}
a3=ans/100
a3

test<-sample(1:6e4)
ans <- 0               
for (i in 1:100) {
  w<-system.time(quick_sort(test))[1]
  ans <- ans + w    
}
a4=ans/100
a4
test<-sample(1:8e4)
ans <- 0               
for (i in 1:100) {
  w<-system.time(quick_sort(test))[1]
  ans <- ans + w    
}
a5=ans/100
a5
an<-c(a1,a2,a3,a4,a5)
tn<-c(1e4*log(1e4),2e4*log(2e4),4e4*log(1e4),6e4*log(6e4),8e4*log(8e4))
model<-lm(an~tn)
plot(tn,an)
abline(model,)
```

Question 2: Exercise 5.6
  
$Cov(e^{U},e^{1-U})=e-(e-1)^2=-0.2342106,Var(e^{U}+e^{1-U})=Var(e^{U})+Var(e^{1-U})+2Cov(e^{U},e^{1-U})=0.01564999$.Before this process,we compute $Var(e^{U})=0.2420351$,so that the percent reduction in variance of $\hat{θ}$ is $(0.2420351-0.01564999)/0.2420351$ $=0.93534$.

Question 2: Exercise 5.7

  In this part,we calculate the empirical estimate of the percent reduction in variance using the antithetic variate is about 0.98 ,which is larger than the theoretical value from Exercise 5.6 $0.93534$.
```{r}
m <- 1e4; x <- runif(m, min=0, max=1)
theta.hat1 <- mean(exp(x))
theta.hat1
theta.hat2 <- mean((exp(x)+exp(1-x))/2)
theta.hat2
var1<-mean(exp(2*x)-(theta.hat1)^2)
var1
var2<-mean((exp(x)+exp(1-x))^2/4-(theta.hat2)^2)
var2
r<-(var1-var2)/var1
r

```

# A-22077-2022-09-30
## Question 

Exercises 5.13, 5.15(pages 149-151, Statistical Computing with R).

There is something wrong with the subintervals in Exercise 5.15 (Example 5.13). You may modify it without losing the original intent.

## Answer

### Exercise 5.13

The integration which we want to estimate is 
$$\int_1^{\infty} \frac{x^2}{\sqrt{2 \pi}} e^{-x^2 / 2} d x.$$
We can use the method of change of variable to change the integration to become another integration:
$$\int_1^{\infty} \frac1 2\frac{t^{1/2}}{\sqrt{2 \pi}} e^{-t/2} d t$$.
Based on the integration which we have transformd,we can choose
$${(\frac1 2)}^{3/2}\frac{(x-1)^{1/2}}{\Gamma(\frac32)} e^{\frac{-x+1}2}$$ as $f_1$,and we choose 

$$\frac12e^{-\frac{x-1}2}$$ as $f_2$.

```{r}
n <- rgamma(1e4,3/2,1/2)+1# the random value of f1
m <- sqrt(2)*n^(1/2)*exp(-n/2)/(n-1)^(1/2)/exp(-(n-1)/2)*gamma(3/2)/sqrt(2*pi)# the random value of g/f1
p <- mean(m)# the mean of samples of g/f1
q <- (m-p)^2
r <- mean(q)# the variance produced by f1
r
```

```{r}
n <- runif(1e4)
n0 <- -2*log(1-n)+1# the random value of f2
m <- exp(-(1/2))/sqrt(2*pi)*n0^(1/2)# the random value of g/f2
p <- mean(m)# the mean of samples of g/f2
q <- (m-p)^2
r <- mean(q)# the variance produced by f2
r
```

Though the r code above,we calculate the variance produced by $f_1$ is much larger than the variance produced by $f_2$.

### Exercise 5.15

We set the CDF of $f_i(x)$ is $F_i(x)$,we choose $\frac{e^{-x}}{1-e^{-1}}$ as $f$,though calculation,we have got the expression  of $F_i(x)$ is 
$$F_i(x)=\frac{5-(1-e^{-1})(i-1)-5e^{-x}}{1-e^{-1}}.$$
And based the expression of $F_i(x)$,we can get the inverse function of $F_i(x)$,we name them as $R_i(x)$,the expression of $R_i(x)$ are as follows:
$$G_i(x)=-log{\frac{5-(1-e^{-1})(i-1)-(1-e^{-1})x}5}.$$
Based on above,we can get the random numbers of $X_i$,which follows $F_i(x)$.And then,we can further get the observed value of $\hat{\theta_i}$ and the estimator of variance of $\hat{\theta_i}$.
```{r}
n <-runif(2000)
n01 <- -log((5-(1-exp(-1))*(1-1)-(1-exp(-1))*n)/5)
n02 <- -log((5-(1-exp(-1))*(2-1)-(1-exp(-1))*n)/5)
n03 <- -log((5-(1-exp(-1))*(3-1)-(1-exp(-1))*n)/5)
n04 <- -log((5-(1-exp(-1))*(4-1)-(1-exp(-1))*n)/5)
n05 <- -log((5-(1-exp(-1))*(5-1)-(1-exp(-1))*n)/5)
m1 <- (1-exp(-1))/(1+n01^2)/5
m2 <- (1-exp(-1))/(1+n02^2)/5
m3 <- (1-exp(-1))/(1+n03^2)/5
m4 <- (1-exp(-1))/(1+n04^2)/5
m5 <- (1-exp(-1))/(1+n05^2)/5
p <- mean(m1)+mean(m2)+mean(m3)+mean(m4)+mean(m5)
r <- mean((m1-mean(m1))^2)+mean((m2-mean(m2))^2)+mean((m3-mean(m3))^2)+mean((m4-mean(m4))^2)+mean((m5-mean(m5))^2)#the variance got by Stratified Importance Sampling
r
```

And then, we can see the result of Example 5.10.
```{r}
m <- 10000
g <- function(x) {
  exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}
u <- runif(m)
x <- - log(1 - u * (1 - exp(-1)))
fg <- g(x) / (exp(-x) / (1 - exp(-1)))
theta.hat <- mean(fg)
r <- mean((fg-theta.hat)^2)#the variance got by Importance Sampling
r
```
 
Thus,we found that the variance got by Stratified Importance Sampling is much smaller than the variance got by Importance Sampling.

# A-22077-2022-10-09
### Question 1


$6.4 \quad$ Suppose that $X_1,...,X_n$ are a random sample from a lognormal distribution with unknown parameters.Construct a 95% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

### Answer 1

Let $Y=log(X),Y \sim N(\mu,\sigma^2)$ thus X obey a lognormal distribution with parameters $\mu,\sigma^2$,

Let $Y_i=log(X_i),i=1,...,n$,thus $Y_1,...,Y_n$ are a random sample with the same distribution of $Y$.

Thus we can get a 95% confidence interval:$(\hat\mu-t_{n-1}(0.95)\hat\sigma/\sqrt{n},\hat\mu+t_{n-1}(0.95)\hat\sigma)/\sqrt{n})$.

Then,We can estimate the confidence level with the following steps:

We set $\mu=0,\sigma^2=1$,sample size n=1000,Number of repeated experiments is m=1000,$\alpha=0.05$ for example:
```{r}
set.seed(22077)
n<-1000
alpha<-0.05
muhat<-numeric(2)
Z<-numeric(1000)#the times of mu in the confidence interval
for(i in 1:1000){
  x<-rlnorm(n)
  muhat[1]<-mean(log(x))-sqrt(var(log(x)))*qt(1-alpha/2,df=n-1)/sqrt(n)
  muhat[2]<-mean(log(x))+sqrt(var(log(x)))*qt(1-alpha/2,df=n-1)/sqrt(n)
  if(muhat[1]<0 && muhat[2]>0){
    Z[i]=1
  }
}
mean(Z)
```
### Question 2

$6.8 \quad$ Refer to Example 6.16.Repeat the simulation,but also compute the F test of equal variance,at significance level $\hat\alpha=0.055$.Compute the power of the Count Five test and F test for small,medium,and large sample sizes,(Recall that F test is not applicable for non-normal distributions.)

### Answer 2

```{r}
count5test <- function(x,y){
  X <- x-mean(x)
  Y <- y-mean(y)
  outx <- (sum(X>max(Y))+ sum(X<min(Y)))
  outy <- (sum(Y>max(X))+ sum(Y<min(X)))
  return(as.integer(max(c(outx,outy))>5))
}#Count Five test
```

```{r}
alpha<-0.055
Ftest <- function(x,y,n){
  varx<-var(x)
  vary<-var(y)
  if(var(x)/var(y)>=qf(alpha/2,n-1,n-1) && var(x)/var(y)<=qf(1-alpha/2,n-1,n-1)){
    return(0)
  }
  else return(1)
}# F test with alpha=0.055 and n1=n2=n
```
We use $N(\mu_1=0,\sigma_1^2=1)$ and $N(\mu_1=0,\sigma_2^2=1.5^2)$ for example.

When $n_1=n_2=20$:

```{r}
set.seed(22077)
m<-10000
n<-20
sigma1<-1
sigma2<-1.5
powercount5<-mean(replicate(m,expr={
  x<-rnorm(n,0,sigma1)
  y<-rnorm(n,0,sigma2)
  count5test(x,y)
}))
# the power of Count Five test
powerF<-mean(replicate(m,expr={
  x<-rnorm(n,0,sigma1)
  y<-rnorm(n,0,sigma2)
  Ftest(x,y,n)
}))
# the power of F test
print(powercount5)
print(powerF)
```

When $n_1=n_2=100$:

```{r}
set.seed(22077)
m<-10000
n<-100
sigma1<-1
sigma2<-1.5
powercount5<-mean(replicate(m,expr={
  x<-rnorm(n,0,sigma1)
  y<-rnorm(n,0,sigma2)
  count5test(x,y)
}))
# the power of Count Five test
powerF<-mean(replicate(m,expr={
  x<-rnorm(n,0,sigma1)
  y<-rnorm(n,0,sigma2)
  Ftest(x,y,n)
}))
# the power of F test
print(powercount5)
print(powerF)
```

When $n_1=n_2=1000$:

```{r}
set.seed(22077)
m<-10000
n<-1000
sigma1<-1
sigma2<-1.5
powercount5<-mean(replicate(m,expr={
  x<-rnorm(n,0,sigma1)
  y<-rnorm(n,0,sigma2)
  count5test(x,y)
}))
# the power of Count Five test
powerF<-mean(replicate(m,expr={
  x<-rnorm(n,0,sigma1)
  y<-rnorm(n,0,sigma2)
  Ftest(x,y,n)
}))
# the power of F test
print(powercount5)
print(powerF)
```
We can see ：

F test always has bigger power than Count Five test;

both of their powers are getting to 1 with the growing of n;

their powers are getting closer with the growing of n.


### Question 3

If we obtain the powers for two methods under a particular simulation setting with 10000 experiments;say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

• What is the corresponding hypothesis test problem?

• Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

• Please provide the least necessary information for hypothesis testing.

### Answer 3

• The corresponding hypothesis test problem should be : $H_0:power_1=power_2$ $H_1:power_1\neq power_2$ 

• Since Z-test is only applicable to the situation of normal distribution with known variance,thus we can not use Z-test;we should use paired-t test or McNemar test because the data is paired.

• We need the powers of two different methods in 10000 experiments and the mean and variance of their difference.

# A-22077-2022-10-14
## 7.4

Refer to the air-conditioning data set $aircondit$ provided in the boot pack-
age.
The 12 observations are the times in hours between failures of air-
conditioning equipment [63, Example 1.1]:
3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.
Assume that the times between failures follow an exponential model $Exp(λ)$.
Obtain the $MLE$ of the hazard rate $λ$ and use bootstrap to estimate the bias
and standard error of the estimate.

STEP1
Use the samples to get the MLE of $\lambda$.
\begin{center}
\begin{equation}

\$L(x_1,x_2,...,x_n;\lambda)=\lambda^n.exp(-{\Sigma_{i=1}^{N}{\lambda{x_i}}})$$ln(L)=nln(\lambda)-{\Sigma_{i=1}^{N}{\lambda{x_i}}}$
$\frac{dln(L)}{d\lambda}=\frac{n}{\lambda}-\Sigma_{i=1}^{N}{{x_i}}=0$ $\hat{\lambda}=\frac{n}{\Sigma_{i=1}^{N}{x_i}}$

\end{equation}

\end{center}


STEP2
Use bootstrap to estimate the bias and standard error of the estimate.

```{r}
y <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
set.seed(22077)
s <- function(y,i){
  mean(y[i])
}
library(boot)
replicate <- boot(data=y,statistic=s,R=1e4)
# use bootstrap to estimate MLE
round(c(theta=replicate$t0,bias=mean(replicate$t)-replicate$t0,se=sd(replicate$t)),4)
```

```{r}
print(boot.ci(replicate,type=c("norm","basic","perc","bca")))
```

Standard normal method requires samples of normal distribution.In some case,which is not accurate;percentile method is more accurate,and BCa method is a improved version of percentile method.

## 7.A

Conduct a Monte Carlo study to estimate the coverage probabilities of the
standard normal bootstrap confidence interval, the basic bootstrap confidence
interval, and the percentile confidence interval. Sample from a normal pop-
ulation and check the empirical coverage rates for the sample mean. Find
the proportion of times that the confidence intervals miss on the left, and the
porportion of times that the confidence intervals miss on the right.

we use $\mu=0$,$\sigma=1$ for example

```{r}
n <- 20;m=1000
sum1 <- numeric(m);sum2 <- numeric(m);sum3 <- numeric(m);sum4 <- numeric(m);
set.seed(22091)
r <- function(x,i){
  mean(x[i])
}
library(boot)
for(i in 1:m){
  x <-rnorm(n)
  obj <- boot(data=x,statistic=r,R=1e4)
  cl <- boot.ci(obj,type=c("norm","basic","perc","bca")) # Compute 95% bootstrap confidence intervals
  if(cl$norm[2]<0 && cl$norm[3]>0){
    sum1[i] <-1
  }
  if(cl$basic[4]<0 && cl$basic[5]>0){
    sum2[i] <-1
  }
  if(cl$perc[4]<0 && cl$perc[5]>0){
    sum3[i] <-1
  }
  if(cl$bca[4]<0 && cl$bca[5]>0){
    sum4[i] <-1
  }
}
round(c(norm=mean(sum1),basic=mean(sum2),perc=mean(sum3),bca=mean(sum4)),3)

```

# A-22077-2022-10-21
## Question{#question}

1. Exercises 7.8 (Page 213, Statistical Computing with R). [Jump to the Answer](#question1ans)

2. Exercises 7.11 (Page 213, Statistical Computing with R). [Jump to the Answer](#question2ans)

3. Exercises 8.2 (Page 242, Statistical Computing with R). [Jump to the Answer](#question3ans)

## Answer

### Exercise 7.8{#question1ans}

**Problem.** Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

**Solution.** First, we write a function to calculate $\hat{\theta}$. Then we can use the jackknife.

```{r}
rm=list(c())
#write the function
pca=function(x,i){
  val=eigen(cov(x[i,]))$values
  return(val[1]/sum(val))
}

library(bootstrap)
n=nrow(scor)
theta.hat <- pca(scor,1:n)
theta.jack <- numeric(n)
for(i in 1:n){
theta.jack[i] <- pca(scor,(1:n)[-i])
}
bias.jack <- (n-1)*(mean(theta.jack)-theta.hat)
se.jack <- sqrt((n-1)*mean((theta.jack-theta.hat)^2))
round(c(original=theta.hat,bias.jack=bias.jack,
se.jack=se.jack),3)
```

[Back to the Question](#question)

### Exercise 7.11{#question2ans}

**Problem.** In Example 7.18, leave-one-out ( $n$-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

**Solution.** 
```{r}
rm=list(c())
library(DAAG)
attach(ironslag)
n=length(magnetic)
e1=e2=e3=e4=c()

for (k in 1:(n-1)) {
  for (l in (k+1):n) {
    y=magnetic[-c(k,l)]
    x=chemical[-c(k,l)]
    
    J1=lm(y~x)
    yhat11=J1$coef[1]+J1$coef[2]*chemical[k]
    yhat12=J1$coef[1]+J1$coef[2]*chemical[l]
    e1=c(e1,magnetic[k]-yhat11,magnetic[l]-yhat12)
    
    J2=lm(y~x+I(x^2))
    yhat21=J2$coef[1]+J2$coef[2]*chemical[k]+J2$coef[3]*chemical[k]^2
    yhat22=J2$coef[1]+J2$coef[2]*chemical[l]+J2$coef[3]*chemical[l]^2
    e2=c(e2,magnetic[k]-yhat21,magnetic[l]-yhat22)
    
    J3=lm(log(y)~x)
    yhat31=exp(J3$coef[1]+J3$coef[2]*chemical[k])
    yhat32=exp(J3$coef[1]+J3$coef[2]*chemical[l])
    e3=c(e3,magnetic[k]-yhat31,magnetic[l]-yhat32)
    
    J4=lm(log(y)~log(x))
    yhat41=exp(J4$coef[1]+J4$coef[2]*log(chemical[k]))
    yhat42=exp(J4$coef[1]+J4$coef[2]*log(chemical[l]))
    e4=c(e4,magnetic[k]-yhat41,magnetic[l]-yhat42)
  }
}
c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2))
detach(ironslag)
```

Comparing the results, we know the model 2 fits the data most.

[Back to the Question](#question)

### Exercise 8.2{#question3ans}

**Problem.** Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the $p$-value reported by cor. test on the same samples.

**Solution.** We generate samples from the standard normal distribution.
```{r}
rm=list(c())
set.seed(11111)
x=rnorm(12)
y=rnorm(12)

R=999
z=c(x,y)
K=1:24
D=numeric(R)
options(warn=-1)
D0=cor(x,y,method = "spearman")
for(i in 1:R){
  k=sample(K,size=12,replace = FALSE)
  x1=z[k]
  y1=z[-k]
  D[i]=cor(x1,y1,method = "spearman")
}
p=mean(c(D0,D)>=D0)
options(warn=0)
p
cor.test(x,y)
```

We can learn that the two p-values are not really different, and they both accept the null hypothesis.

[Back to the Question](#question)

--------

# A-22077-2022-10-28
## Question 1

$ 9.4\quad$ 
Implement a random walk Metropolis sampler for generating the standard
Laplace distribution (see Exercise 3.2). For the increment, simulate from a
normal distribution. Compare the chains generated when different variances
are used for the proposal distributon. Also, compute the acceptance rates of
each chain.And use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to ˆR < 1.2.

## Answer 1

use proposal distribution $N(X_t,\sigma^2)$ to generate with four different sigma
 0.05, 0.5, 2, 16.
```{r}
set.seed(22077)
# use the function below to calculate the density of  Laplace distribution
f <- function(x){
  return(exp(-abs(x))/2)
}
# use the function below to generate chain with given parameters:sigma,n,X_0,N
mh <- function(sigma,N,x0){
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for(i in 2:N){
    xt <- x[i-1]
    y <- rnorm(1,xt,sigma)
    num <- f(y)*dnorm(xt,y,sigma)
    den <- f(xt)*dnorm(y,xt,sigma)
    if(u[i] <= num/den) x[i] <- y else{
      x[i] <- xt 
      k <- k+1 # y is rejected
    }
  }
  return(list(x=x,k=k))
}

# generate chains with four differeint sigma

N <- 15000
sigma <- c(.05, .5, 2, 16)
x0 <- 25
mh1 <- mh(sigma[1],N,x0)
mh2 <- mh(sigma[2],N,x0)
mh3 <- mh(sigma[3],N,x0)
mh4 <- mh(sigma[4],N,x0)
# calculate the acceptance rates
print(c((N-mh1$k)/N,(N-mh2$k)/N,(N-mh3$k)/N,(N-mh4$k)/N))
```
we can see that the acceptance rates become smaller with the growing of sigma

use the Gelman-Rubin method to monitor convergence of the chain,we use sigma =0.5 for example
```{r}
Gelman.Rubin <- function(psi){
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i_th row of X
  pis <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)
  B <- n*var(psi.means)
  psi.w <- apply(psi,1,"var")
  W <- mean(psi.w)
  v.hat <- W*(n-1)/n + (B/n)
  r.hat <- v.hat / W            # G-R statistic
  return(r.hat)
}

k <- 4
b <- 1000
sigma <- 0.5

# choose overdispersed initial values
x0 <- c(-10,-5,5,10)

# generate the chains
X <- matrix(0,nrow=k,ncol=N)
for(i in 1:k){
  X[i,] <- mh(sigma,N,x0[i])$x
}

# compute diagnostic statistics
psi <- t(apply(X,1,cumsum))
for(i in 1:nrow(psi))
  psi[i,] <- psi[i,]/(1:ncol(psi))
print(Gelman.Rubin(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0,N)
for(j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N],type="l",xlab="",ylab="R")
abline(h=1.2,lty=2)
```

we can see  when N=15000,the $\hat R$ is smaller than 1.2, and when N is about 6000,the $\hat R$ is about 1.2,which means the chain  converges approximately to the target distribution


## Queston 2

$ 9.7 \quad$ Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard deviations, and correlation 0.9. Plot the
generated sample after discarding a suitable burn-in sample. Fit a simple
linear regression model Y = β0 + β1X to the sample and check the residuals
of the model for normality and constant variance.And use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to ˆR < 1.2.

## Answer 2

```{r}
# initialize constants and parameters
N <- 5000          # length of chain
burn <- 1000       # burn-in length
X <- matrix(0,N,2) # the chain, a bivariate sample

rho <- 0.9         # correlation
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
s1 <- sqrt(1-rho^2)*sigma1
s2 <- sqrt(1-rho^2)*sigma2

##### generate the chain #####

X[1, ] <- c(mu1,mu2)  # initialize
for(i in 2:N){
  x2 <- X[i-1,2]
  m1 <- mu1 + rho*(x2-mu2)*sigma1/sigma2
  X[i,1] <- rnorm(1,m1,s1)
  x1 <- X[i,1]
  m2 <- mu2 + rho*(x1-mu1)*sigma2/sigma1
  X[i,2] <- rnorm(1,m2,s2)
}

# Remove the first 1000 observations and draw the graph generated by the sample
b <- burn + 1
x <- X[b:N, ]

plot(x,main="",cex=.5,xlab=bquote(X[1]),ylab=bquote(X[2]),ylim=range(X[,2]))

lm <- lm(x[,1]~x[,2])
summary(lm)
qqnorm(x)
```
we can see p-value is very small and the fitting is very good. The mean and variance are close to the real parameters

```{r}
Gelman.Rubin <- function(psi){
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i_th row of X
  pis <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  
  psi.means <- rowMeans(psi)
  B <- n*var(psi.means)
  psi.w <- apply(psi,1,"var")
  W <- mean(psi.w)
  v.hat <- W*(n-1)/n + (B/n)
  r.hat <- v.hat / W            # G-R statistic
  return(r.hat)
}

k <- 4
b <- 1000
sigma <- 0.6

# choose overdispersed initial values
x0 <- c(-10,-5,5,10)

# generate the chains
X <- matrix(0,nrow=k,ncol=N)
for(i in 1:k){
  X[i,] <- mh(sigma,N,x0[i])$x
}

# compute diagnostic statistics
psi <- t(apply(X,1,cumsum))
for(i in 1:nrow(psi))
  psi[i,] <- psi[i,]/(1:ncol(psi))
print(Gelman.Rubin(psi))

#plot the sequence of R-hat statistics
rhat <- rep(0,N)
for(j in (b+1):N)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):N],type="l",xlab="",ylab="R")
abline(h=1.2,lty=2)
```

# A-22077-2022-11-04
## Question 1

Set up a random simulation study to examine the performance of the above three permutation test methods

## Answer 1

consider model $$M=a_M+\alpha X+e_M$$
and $$ Y=a_Y+ \beta M +\gamma X+e_Y$$
Consider three parameter combinations: $\alpha=0,\beta=0$;$\alpha=0,\beta=1$;
$\alpha=1,\beta=0$ ,and use permutation test.

Compare the quality of the test by comparing the p value

## Question 2

Consider model $$ P(Y=1|x1,x2,x3)=expit(\alpha+b_1x_1+b_2x_2+b_3x_3)$$
with $x_1~P(1),x_2~Exp(1),x_3~B(1,0.5)$ write a function to calculate alpha with 
N,b1,x1,x2,x3,f0. And use the function with N=1e6,b1=0,b2=1,b3=-1,f0=0.1, 0.01, 0.001, 0.0001,  and draw f0 vs alpha

## Answer 2

```{r}
set.seed(22077)
root <- function(N,b1,b2,b3,f0){
  x1 <- rpois(N,1); x2 <- rexp(N,1); x3 <- rbinom(N,1,0.5)
  g <- function(alpha){
    tmp <- exp(alpha+b1*x1+b2*x2+b3*x3)
    p <- 1/(1+tmp)
    mean(p) - f0
    }
  solution <- uniroot(g,c(0,10))
  solution$root
}
N <- 1e6; b1 <- 0; b2 <- 1; b3  <- -1; f0 <-c(.1,.01,.001,.0001)
alpha <- numeric(4)
for(i in 1:4){
  alpha[i] <- root(N,b1,b2,b3,f0[i])
}
alpha
plot(f0,alpha,xlim=c(0,0.1),ylim=c(0,10))
```

# A-22077-2022-11-11
## Question 1

let  $X_1,X_2,...,X_n$ $i.i.d.\sim$  $Exp(\lambda)$. We know that $X_i$ is located in ($u_i,v_i$), where $u_,v_i$ are two known certain constants.
(1)maximize the likelihood function of observated data and use EM algorithm to solve the $MLE$ of $\lambda$respectively, and prove that they are equal.
(2)Set the observation value of $(u_i,v_i)$ as $(11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3)$.Use the two methods to calculate MLE.

## Answer 1

The likelihood function is $$ L(\lambda)=\prod_{i=1}^{n}(e^{-\lambda u_i}-e^{-\lambda v_i})$$
thus $$ l(\lambda)=ln L(\lambda) =\sum_{i=1}^{n} ln (e^{-\lambda u_i}-e^{-\lambda v_i})$$
and let $\partial l(\lambda)/ \partial \lambda =0$, then we can get $\hat\lambda$

program to calculate $\hat\lambda$
```{r}
u <- c(11,8,27,13,16,0,23,10,24,2)
v <- c(12,9,28,14,17,1,24,11,25,3)
x <- numeric(10)
obj <- function(lambda,u,v){
  for(i in 1: 10){
    x[i] <- (v[i]-u[i])/(exp(lambda*(v[i]-u[i])))-u[i]
  }
  sum(x)
}
u <- uniroot(obj,lower=-10,upper=10,u= u,v= v)
lambda.hat <- u$root
lambda.hat
```

## Question 2

Why do you need to use unlist() to convert a list to an atomic
vector? Why doesn’t as.vector() work?

## Answer 2

Because all elements of an atomic vector must be the same type, whereas the elements of a list can have different types. If we use as.vector,we may lose some informations.

## Question 3

Why is 1 == "1" true? Why is -1 < FALSE true? Why is "one"
< 2 false?

## Answer 3

When we attempt to combine different types they will be coerced to the most
flexible type.
when we use "==", 1 will be coerced to  "1",so it is correct;

When a logical vector is coerced to an integer or double, TRUE becomes 1
and FALSE becomes 0,so when we  use  -1< FALSE,it is same with -1<0, so it is true; 
"one" is character type and it is more flexible than double type, so "one" < 2 is false.

## Question 4

What does "dim()" return when applied to a vector?

## Answer 4

It will return NULL;if we want to get the length of a vector, we should use "length()".

## Question 5

If is.matrix(x) is TRUE, what will is.array(x) return?

## Answer 5

It will return TRUE,because a matrix is also an array.

## Question 6

What attributes does a data frame possess?

## Answer 6

Column names should  not be empty.

Row names should be unique.

The data stored in the data frame can be of number, factor or character type.

Each column should contain the same number of data items.

## Question 7

What does as.matrix() do when applied to a data frame with columns of different types?

## Answer 7

It will return a matrix with the data of the data frame.

## Question 8

Can you have a data frame with 0 rows? What about 0 columns?

## Answer 8

If we create a data frame with nothing,it will show that we get a data frame with 0 rows.

# A-22077-2022-11-11
# Question 1

The function below scales a vector so it falls in the range [0,
1]. How would you apply it to every column of a data frame?
How would you apply it to every numeric column in a data
frame?

# Answer 1

## a)
```{r}
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
beauty<-data.frame(x=0:4,y=6:10)
lapply(beauty,scale01)
```
## b)
```{r}
beauty<-data.frame(x=c(0,0.5,0.7,0.9,0.5),y=c(6,7,8,9,10),z=c('a','b','c','d','e'))
a = c(0)
  for (i in 1:ncol(beauty)) {
    if (class(beauty[,i])!='numeric') a=c(a,i) 
  }
beauty1 = beauty[,-a]
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
lapply(beauty1,scale01)
```
# Question 2

Use vapply() to:
a) Compute the standard deviation of every column in a nu-
meric data frame.
b) Compute the standard deviation of every numeric column
in a mixed data frame. (Hint: you’ll need to use vapply()
twice.)

## a)
```{r}
beauty<-data.frame(x=c(0,0.5,0.7,0.9,0.5),y=c(6,7,8,9,10))
vapply(beauty,sd,c(1))
```

## b)

```{r}
beauty<-data.frame(x=c(0,0.5,0.7,0.9,0.5),y=c(6,7,8,9,10),z=c('a','b','c','d','e'))
a <-vapply(beauty,is.numeric,c(1))
b <-c(0)
  for (i in 1:length(a)) {
    if (class(a[i])!='numeric') b=c(b,i) 
  }
beauty1 = beauty[,-b]
vapply(beauty1,sd,c(1))
```
# Question 3
Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard
deviations, and correlation 0.9.
• Write an Rcpp function.
• Compare the corresponding generated random numbers with pure R language using the function
“qqplot”.
• Compare the computation time of the two functions with the function “microbenchmark”.

# Answer 3
```{r}
library(Rcpp)
dir_cpp <- '../Rcpp/'
# Can create source file in Rstudio
sourceCpp('D:/meanC.cpp')
library(microbenchmark)
x <- runif(1e4); mean2 <- function(x)sum(x)/length(x)
ts <- microbenchmark(meanR=mean(x),meanR2=mean2(x),
meancpp=meanC(x))
summary(ts)[,c(1,3,5,6)]

```
```{r}
N <- 5000
burn <- 1000 # burn-in length
X <- matrix(0,N,2)

rho <-0.9 # correlation
mu1 <- mu2 <-0
sigma1 <- sigma2 <-1


# generate the chain
gibbs_r <- function(N,burn,X,rho,mu1,mu2,sigma1,sigma2){
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
  X[1,] <- c(mu1,mu2)
  for(i in 2:N){
    x2 <- X[i-1,2]
    m1 <- mu1+rho*(x2-mu2)*sigma1/sigma2
    X[i,1] <- rnorm(1,m1,s1)
    x1 <- X[i,1]
    m2 <- mu2+rho*(x1-mu1)*sigma2/sigma1
    X[i,2] <- rnorm(1,m2,s2)
  }
  
  b <- burn +1 
  x <- X[b:N,]
}

x1 <- gibbs_r(N,burn,X,rho,mu1,mu2,sigma1,sigma2)
```

use Rcpp to generate:
```{r}
N <- 5000
burn <- 1000 # burn-in length
X <- matrix(0,N,2)

rho <-0.9 # correlation
mu1 <- mu2 <-0
sigma1 <- sigma2 <-1
library(Rcpp)
sourceCpp('D:/gibbs_cpp.cpp')
x2 <- gibbs_cpp(N,mu1,mu2,sigma1,sigma2,rho)
```

Compare the corresponding generated random numbers with pure R language using the function “qqplot”

```{r}
qqplot(x1,x2)
```

Compare the computation time of the two functions with the function “microbenchmark”

```{r}
library(microbenchmark)
microbenchmark(gibbs_r(N,burn,X,rho,mu1,mu2,sigma1,sigma2),gibbs_cpp(N,mu1,mu2,sigma1,sigma2,rho))
```
we can see gibbs_cpp use less time.


